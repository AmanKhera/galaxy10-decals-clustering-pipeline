{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfec2f7-f6f1-41a6-9498-73a7bbf58eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB3, preprocess_input\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB4\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import gc\n",
    "from tensorflow import keras as K\n",
    "from tensorflow.keras import layers as L\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "tf.config.experimental.enable_tensor_float_32_execution(True)\n",
    "\n",
    "tf.config.optimizer.set_jit(True)\n",
    "mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb93803c-f143-41f9-b0ac-ecc021e7ca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View dataset distribution\n",
    "\n",
    "H5_PATH = \"images/Galaxy10_DECals.h5\"\n",
    "LABEL_KEY = \"ans\" \n",
    "\n",
    "def main():\n",
    "    with h5py.File(H5_PATH, \"r\") as f:\n",
    "        y = f[LABEL_KEY][:]  # read labels only\n",
    "\n",
    "    # Handle common label formats:\n",
    "    # 1) integer class ids: shape (N,)\n",
    "    # 2) one-hot / multi-class probs: shape (N, C)\n",
    "    y = np.asarray(y)\n",
    "    if y.ndim == 2:\n",
    "        # one-hot or probabilities -> convert to class ids\n",
    "        y_ids = np.argmax(y, axis=1).astype(np.int64)\n",
    "        num_classes = y.shape[1]\n",
    "    elif y.ndim == 1:\n",
    "        y_ids = y.astype(np.int64)\n",
    "        num_classes = int(y_ids.max()) + 1\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected label shape: {y.shape}\")\n",
    "\n",
    "    counts = np.bincount(y_ids, minlength=num_classes)\n",
    "    total = counts.sum()\n",
    "\n",
    "    print(f\"Total samples: {total}\\n\")\n",
    "    print(\"Class distribution:\")\n",
    "    for c, n in enumerate(counts):\n",
    "        pct = (n / total * 100.0) if total else 0.0\n",
    "        print(f\"  Class {c:>2}: {n:>7}  ({pct:>6.2f}%)\")\n",
    "\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure()\n",
    "        plt.bar(np.arange(num_classes), counts)\n",
    "        plt.xlabel(\"Class\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.title(\"Galaxy10_DECals class distribution\")\n",
    "        plt.xticks(np.arange(num_classes))\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d46900-09b7-4fe9-bd5a-40f765f5e68b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#View a small sample\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#load data\n",
    "import h5py\n",
    "with h5py.File(\"images/Galaxy10_DECals.h5\", \"r\") as f:\n",
    "    X = f[\"images\"][:]\n",
    "    y = f[\"ans\"][:]\n",
    "\n",
    "label_names = [\n",
    "    \"Disturbed\", \"Merging\", \"Round Smooth\", \"In-between Round Smooth\", \"Cigar\",\n",
    "    \"Barred Spiral\", \"Tight Spiral\", \"Loose Spiral\", \"Edge-on (no bulge)\", \"Edge-on (with bulge)\"\n",
    "]\n",
    "\n",
    "def show_random_grid(X, y, nrows=3, ncols=5, seed=None):\n",
    "    n = nrows * ncols\n",
    "    if len(X) < n:\n",
    "        raise ValueError(f\"Need at least {n} images, but got {len(X)}\")\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = rng.choice(len(X), size=n, replace=False)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(3*ncols, 3*nrows))\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "\n",
    "    for ax, i in zip(axes, idx):\n",
    "        img = X[i]\n",
    "        ax.imshow(img)\n",
    "        label = int(y[i])\n",
    "        title = f\"{label}: {label_names[label]}\" if 0 <= label < len(label_names) else str(label)\n",
    "        ax.set_title(title, fontsize=10)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_random_grid(X, y, nrows=3, ncols=5, seed=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce5a542-d6b7-4cea-804d-bb8971be6e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "\n",
    "with h5py.File(\"images/Galaxy10_DECals.h5\", \"r\") as f:\n",
    "    X = f[\"images\"][:]   # uint8 array in RAM (~3.3GB)\n",
    "    y = f[\"ans\"][:]      # labels\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((X,y))\n",
    "\n",
    "ds_size = len(ds)\n",
    "ds = ds.shuffle(buffer_size = ds_size, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "#Splitting dataset\n",
    "train_size = int(0.8 * ds_size)\n",
    "val_size   = int(0.1 * ds_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1659b28e-722e-4b30-a548-d0c3bb4d9c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting dataset\n",
    "train_size = int(0.8 * ds_size)\n",
    "val_size   = int(0.1 * ds_size)\n",
    "\n",
    "train_ds = ds.take(train_size)\n",
    "rest_ds  = ds.skip(train_size)\n",
    "val_ds   = rest_ds.take(val_size)\n",
    "test_ds  = rest_ds.skip(val_size)\n",
    "\n",
    "# (optional) free big arrays\n",
    "del X, y\n",
    "gc.collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c301fd-47aa-43ec-ad24-1788feaf0118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Class weights and boost disturbed class\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# y is scalar (shape ()), so collect into 1D array\n",
    "y_train_ids = np.fromiter((int(y.numpy()) for _, y in train_ds),\n",
    "                          dtype=np.int32,\n",
    "                          count=train_size)\n",
    "\n",
    "counts = np.bincount(y_train_ids, minlength=NUM_CLASSES)\n",
    "\n",
    "w = counts.sum() / (NUM_CLASSES * counts)          # inverse freq\n",
    "w = np.sqrt(w).astype(np.float32)                  # soften\n",
    "\n",
    "\n",
    "DISTURBED_ID = 0\n",
    "LOOSE_SPIRAL_ID = 7\n",
    "\n",
    "DISTURBED_BOOST = 1.1   # start here; try 1.25–2.0\n",
    "LOOSE_SPIRAL_BOOST = 1.15\n",
    "w[LOOSE_SPIRAL_ID] *= LOOSE_SPIRAL_BOOST\n",
    "w[DISTURBED_ID] *= DISTURBED_BOOST\n",
    "\n",
    "w = np.clip(w, 0.7, 2.0).astype(np.float32)        # tighter clip\n",
    "\n",
    "cw = tf.constant(w, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6945f3-d805-46f3-af96-9bcb6e1ee6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2652f3da-fd6b-4024-a726-b06552e74d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "H, W = 256, 256 #before resizing needed for strong augment\n",
    "\n",
    "def add_sample_weight(x, y):\n",
    "    y_id = tf.argmax(y, axis=-1)          # one-hot -> id\n",
    "    w = tf.gather(cw, y_id)               # weight for that class\n",
    "    return x, y, w\n",
    "\n",
    "def to_onehot(x, y):\n",
    "    # x = tf.cast(x, tf.float32)\n",
    "    y = tf.one_hot(tf.cast(y, tf.int32), depth=NUM_CLASSES)\n",
    "    return x, y    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f4e183-3471-4db5-865f-c47d0c2c9d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = [300, 300]\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 48\n",
    "DISTURBED_ID = 0\n",
    "\n",
    "\n",
    "#Oversampling disturbed Galaxies\n",
    "\n",
    "with h5py.File(\"disturbed_oversampled.h5\", \"r\") as f:\n",
    "    X_disturbed = f[\"images\"][:]\n",
    "    y_disturbed = f[\"ans\"][:]\n",
    "\n",
    "\n",
    "# Create dataset from oversampled disturbed examples\n",
    "disturbed_ds = tf.data.Dataset.from_tensor_slices((X_disturbed, y_disturbed))\n",
    "# Combine with original training set\n",
    "# train_ds is already split from your original code\n",
    "train_ds_combined = train_ds.concatenate(disturbed_ds)\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "train_ds_combined = train_ds_combined.shuffle(\n",
    "    buffer_size=len(train_ds_combined), \n",
    "    seed=42, \n",
    "    reshuffle_each_iteration=False\n",
    ")\n",
    "\n",
    "print(f\"Original training size: {train_size}\")\n",
    "print(f\"Combined training size: {train_size + len(X_disturbed)}\")\n",
    "\n",
    "# Clean up\n",
    "del X_disturbed, y_disturbed\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "train_ds_w = (train_ds_combined\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    # .map(extra_aug_only_disturbed, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .map(to_onehot, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .map(add_sample_weight, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "val_ds = (val_ds\n",
    "          .map(to_onehot, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "          .batch(BATCH_SIZE)\n",
    "          .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "test_ds = (test_ds\n",
    "           .map(to_onehot, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "           .batch(BATCH_SIZE)\n",
    "           .prefetch(tf.data.AUTOTUNE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00452ac9-6b8a-4c8a-b592-e5d85d19668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = next(iter(train_ds.take(1)))\n",
    "print(\"x min/max:\", tf.reduce_min(xb).numpy(), tf.reduce_max(xb).numpy())\n",
    "print(\"x dtype:\", xb.dtype, \"y shape:\", yb.shape)\n",
    "\n",
    "\n",
    "xb, yb, wb = next(iter(train_ds_w.take(1)))\n",
    "print(\"x:\", xb.shape, xb.dtype)\n",
    "print(\"y:\", yb.shape, yb.dtype)\n",
    "print(\"w:\", wb.shape, wb.dtype, \"min/max:\", float(tf.reduce_min(wb)), float(tf.reduce_max(wb)))\n",
    "\n",
    "xb2, yb2 = next(iter(val_ds.take(1)))\n",
    "print(\"val y:\", yb2.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e0964e-7918-4afc-812b-e17ece773ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = tf.data.Options()\n",
    "options.autotune.enabled = True\n",
    "options.experimental_optimization.map_parallelization = True\n",
    "options.experimental_optimization.parallel_batch = True\n",
    "train_ds_w = train_ds_w.with_options(options)\n",
    "val_ds = val_ds.with_options(options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ff208f-91b6-4954-885c-6bd52a402082",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load encoder\n",
    "\n",
    "with tf.device('/gpu:0'): #load into gpu\n",
    "    pretrained_model = tf.keras.applications.EfficientNetB4(\n",
    "        weights = 'imagenet', # use pre-trained weights\n",
    "        include_top = False, # we are creating the head below\n",
    "        # input_shape = [*IMAGE_SIZE, 3]\n",
    "        input_shape = [380,380 , 3]\n",
    "    )\n",
    "\n",
    "    pretrained_model.trainable = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079d63dc-75bc-4594-902f-81e1fbd72ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Architecture\n",
    "\n",
    "@K.utils.register_keras_serializable()\n",
    "class CastToFloat32(K.layers.Layer):\n",
    "    def call(self, x):\n",
    "        x = tf.cast(x, tf.float32)\n",
    "        return preprocess_input(x)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inputs = tf.keras.Input(shape=(256,256,3))\n",
    "#Removed due to not being compatible with jit_compile\n",
    "# x = tf.keras.layers.RandomFlip(mode=\"horizontal_and_vertical\", seed=42)(inputs)\n",
    "# x = tf.keras.layers.RandomRotation(0.08, fill_mode=\"reflect\", interpolation=\"bilinear\", seed=42)(x)\n",
    "# x = tf.keras.layers.RandomBrightness(factor=0.1, value_range=(0, 255),seed=42)(x)\n",
    "# x = tf.keras.layers.RandomContrast(factor=0.2, seed=42)(x)\n",
    "\n",
    "\n",
    "x = tf.keras.layers.Resizing(380, 380, interpolation='bilinear')(inputs)\n",
    "\n",
    "\n",
    "#Moved augmentation to model to increase work done on GPU\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\", dtype=\"float32\"),\n",
    "    tf.keras.layers.RandomBrightness(0.25, value_range=(0.0, 255.0), dtype=\"float32\"),\n",
    "    tf.keras.layers.RandomContrast(0.2, dtype=\"float32\"),\n",
    "    tf.keras.layers.RandomErasing(factor=0.25, value_range=(0.0, 255.0), dtype=\"float32\"),\n",
    "    tf.keras.layers.RandomGaussianBlur(factor=0.15, value_range=(0.0, 255.0), dtype=\"float32\")\n",
    "], name=\"aug\")\n",
    "\n",
    "x = CastToFloat32(name=\"to_fp32\")(x)\n",
    "x = data_augmentation(x)\n",
    "\n",
    "\n",
    "\n",
    "x = pretrained_model(x, training=False) #pass inputs through pretrained_model\n",
    "\n",
    "x = L.Conv2D(128, 1, padding = 'same', use_bias=False)(x) #compress 1536 channels down (last conv layer in B3) to 256\n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.ReLU()(x)\n",
    "\n",
    "x = L.Conv2D(128, 3, padding = 'same', use_bias=False)(x) \n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.ReLU()(x)\n",
    "\n",
    "x = L.Conv2D(128, 3, padding = 'same', use_bias=False)(x)\n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.ReLU()(x)\n",
    "\n",
    "\n",
    "\n",
    "x = L.Conv2D(64, 3, padding = 'same', use_bias=False)(x)\n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.ReLU()(x)\n",
    "\n",
    "\n",
    "x = L.Conv2D(64, 3, padding = 'same', use_bias=False)(x) \n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.ReLU()(x)\n",
    "\n",
    "\n",
    "x = L.Conv2D(64, 3, padding = 'same', use_bias=False)(x)\n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.ReLU()(x)\n",
    "\n",
    "\n",
    "x = L.Conv2D(48, 1, padding = 'same', use_bias=False)(x) \n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.ReLU()(x)\n",
    "\n",
    "\n",
    "x = L.Conv2D(48, 3, padding = 'same', use_bias=False)(x) \n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.ReLU()(x)\n",
    "\n",
    "\n",
    "x = L.Conv2D(48, 3, padding = 'same', use_bias=False)(x) \n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.ReLU()(x)\n",
    "\n",
    "x = L.Conv2D(32, 1, padding = 'same', use_bias=False)(x) \n",
    "x = L.BatchNormalization()(x)\n",
    "x = L.ReLU()(x)\n",
    "\n",
    "\n",
    "x = L.GlobalAveragePooling2D()(x)\n",
    "x = L.Dropout(0.35)(x)\n",
    "\n",
    "outputs = L.Dense(10, activation='softmax', dtype='float32')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7771a0-e2b3-4523-9f1d-fd14d9c9bc0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=K.optimizers.Adam(1e-4),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n",
    "    metrics=['categorical_accuracy'],\n",
    "    jit_compile=True\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3454848-e1f7-41dd-81fa-5d30a0884a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "class TQDMBar(Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        total = self.params[\"epochs\"] - self.params.get(\"initial_epoch\", 0)\n",
    "        self.pbar = tqdm(total=total)\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.pbar.update(1)\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.pbar.close()\n",
    "\n",
    "#Base list of callbacks to manage LR, early stopping, and saving the best model\n",
    "cbs = [\n",
    "  tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', #Watch val_loss\n",
    "                                       factor=0.5, # when plateauing, multiply LR by 0.5\n",
    "                                       patience=2, # wait 2 epochs of no val_loss improvement before reducing LR\n",
    "                                       min_lr=5e-6, # dont reduce LR below this minimum\n",
    "                                       verbose=1),\n",
    "  tf.keras.callbacks.EarlyStopping(\n",
    "                                  monitor='val_loss', # Stop based on validation loss\n",
    "                                   patience=4, # stop if no improvement for 4 epochs\n",
    "                                   restore_best_weights=True # revert to best weights seen during training\n",
    "                                  ),\n",
    "  tf.keras.callbacks.ModelCheckpoint(\n",
    "      'best.keras', # where the best model will be saved plus name\n",
    "      monitor='val_loss', # pick \"best\" by lowest validation loss\n",
    "      save_best_only=True # only overwrite file when val_loss improves\n",
    "  )\n",
    "]\n",
    "\n",
    "\n",
    "#View training metrics\n",
    "logdir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tb = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=logdir,\n",
    "    profile_batch=(370, 400)  \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#all Callbacks combined\n",
    "all_callbacks = cbs + [TQDMBar()] + [tb]\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds_w, # Training dataset\n",
    "    validation_data=val_ds, # Validation set\n",
    "    epochs=EPOCHS, #number of epochs to run\n",
    "    callbacks=all_callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1afa49-d568-4a58-a689-435407289319",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Unfreezing layers in backbone and starting state 2 fine-tuning\n",
    "\n",
    "FT_EPOCHS = 20  #fine-tune epochs for stage 2\n",
    "\n",
    "#steps/epoch for cosine schedule\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds_w).numpy()\n",
    "\n",
    "#Handle special cardinality values\n",
    "if steps_per_epoch == tf.data.experimental.INFINITE_CARDINALITY:\n",
    "    raise ValueError(\"train_ds has infinite cardinality; cannot infer steps_per_epoch.\")\n",
    "if steps_per_epoch < 0:\n",
    "    raise ValueError(\n",
    "        \"train_ds cardinality is UNKNOWN. \"\n",
    "        \"Compute steps_per_epoch manually as ceil(num_train_examples / BATCH_SIZE).\"\n",
    "    )\n",
    "\n",
    "total_decay_steps = steps_per_epoch * FT_EPOCHS\n",
    "print(\"steps_per_epoch:\", steps_per_epoch)\n",
    "print(\"total_decay_steps:\", total_decay_steps)\n",
    "\n",
    "\n",
    "pretrained_model.trainable = True\n",
    "\n",
    "for layer in pretrained_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "#Unfreezing last 150 layers\n",
    "for layer in pretrained_model.layers[-50:]:\n",
    "    if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "        layer.trainable = True\n",
    "\n",
    "# ---- cosine decay learning rate ----\n",
    "lr0 = 1e-5\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=lr0,\n",
    "    decay_steps=total_decay_steps,\n",
    "    alpha=0.05,  # ends at 5% of lr0\n",
    ")\n",
    "\n",
    "optimizer = K.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n",
    "    metrics=[tf.keras.metrics.CategoricalAccuracy(name=\"acc\")],\n",
    "    jit_compile=True\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "logdir2 = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tb2 = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=logdir,\n",
    "    profile_batch=(370, 400) \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cbs2 = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\"best_stage2_cosine.keras\", monitor=\"val_loss\", save_best_only=True),\n",
    "    TQDMBar(),\n",
    "    tb2,\n",
    "]\n",
    "\n",
    "#Continuing from stage 1\n",
    "initial_epoch = len(history.history[\"loss\"])\n",
    "history2 = model.fit(\n",
    "    train_ds_w,\n",
    "    validation_data=val_ds,\n",
    "    epochs=initial_epoch + FT_EPOCHS,\n",
    "    initial_epoch=initial_epoch,\n",
    "    callbacks=cbs2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f489089d-a666-4c56-8bb3-164f14aa4e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View training metrics\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a252d6b-70a0-4ea1-9724-f24324599c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"galaxy_b3_fixed.keras\")                 # full model, reloadable\n",
    "model.save_weights(\"galaxy_b3_fixed.weights.h5\")    # weights-only escape hatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b6c88e-6e33-4853-8968-eb6791146ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c320d1-225a-43d2-8a36-f3067219493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from sklearn.metrics import (\n",
    "    classification_report, f1_score, precision_score, recall_score, accuracy_score\n",
    ")\n",
    "\n",
    "\n",
    "class_names = [\n",
    "    \"Disturbed\", \"Merging\", \"Round Smooth\", \"In-between Round Smooth\", \"Cigar\",\n",
    "    \"Barred Spiral\", \"Tight Spiral\", \"Loose Spiral\", \"Edge-on (no bulge)\", \"Edge-on (with bulge)\"\n",
    "]\n",
    "\n",
    "#Collect y_true from the dataset\n",
    "y_true_raw = np.concatenate([y.numpy() for _, y in test_ds], axis=0)\n",
    "\n",
    "#Convert one-hot -> class ids\n",
    "if y_true_raw.ndim == 2:\n",
    "    y_true = np.argmax(y_true_raw, axis=1)\n",
    "else:\n",
    "    y_true = y_true_raw.astype(int)\n",
    "\n",
    "#Get model predictions\n",
    "y_prob = model.predict(test_ds, verbose=0)\n",
    "\n",
    "#Adjust threshold for disturbed class\n",
    "DISTURBED_THRESHOLD = 0.58\n",
    "y_pred_standard = np.argmax(y_prob, axis=1)\n",
    "y_pred = y_pred_standard.copy()\n",
    "\n",
    "disturbed_mask = (y_pred_standard == DISTURBED_ID)\n",
    "disturbed_probs = y_prob[disturbed_mask, DISTURBED_ID]\n",
    "low_confidence = disturbed_probs < DISTURBED_THRESHOLD\n",
    "disturbed_indices = np.where(disturbed_mask)[0]\n",
    "low_confidence_indices = disturbed_indices[low_confidence]\n",
    "\n",
    "for idx in low_confidence_indices:\n",
    "    probs_sorted = np.argsort(y_prob[idx])[::-1]\n",
    "    y_pred[idx] = probs_sorted[1]\n",
    "\n",
    "print(f\"Using disturbed threshold: {DISTURBED_THRESHOLD}\")\n",
    "print(f\"Disturbed predictions before threshold: {np.sum(y_pred_standard == DISTURBED_ID)}\")\n",
    "print(f\"Disturbed predictions after threshold: {np.sum(y_pred == DISTURBED_ID)}\")\n",
    "print(f\"Predictions changed: {np.sum(y_pred != y_pred_standard)}\\n\")\n",
    "\n",
    "# ============= ANALYZE TRUE POSITIVES (CORRECT DISTURBED PREDICTIONS) =============\n",
    "disturbed_true_mask = (y_true == DISTURBED_ID)\n",
    "disturbed_pred_mask = (y_pred == DISTURBED_ID)\n",
    "true_positive_mask = disturbed_true_mask & disturbed_pred_mask\n",
    "\n",
    "# Get indices of true positives\n",
    "tp_indices = np.where(true_positive_mask)[0]\n",
    "\n",
    "# Calculate statistics for true positives\n",
    "tp_disturbed_probs = y_prob[tp_indices, DISTURBED_ID]\n",
    "\n",
    "print(f\"=== True Positive Analysis (Correct Disturbed Predictions) ===\")\n",
    "print(f\"Total true positives: {len(tp_indices)}\")\n",
    "print(f\"\\n=== Disturbed Probability Statistics for True Positives ===\")\n",
    "print(f\"Mean: {np.mean(tp_disturbed_probs):.4f}\")\n",
    "print(f\"Median: {np.median(tp_disturbed_probs):.4f}\")\n",
    "print(f\"Min: {np.min(tp_disturbed_probs):.4f}\")\n",
    "print(f\"Max: {np.max(tp_disturbed_probs):.4f}\")\n",
    "print(f\"Std: {np.std(tp_disturbed_probs):.4f}\")\n",
    "\n",
    "# Store true positive data\n",
    "tp_data = []\n",
    "for idx in tp_indices:\n",
    "    disturbed_prob = y_prob[idx, DISTURBED_ID]\n",
    "    second_best_class = np.argsort(y_prob[idx])[-2]\n",
    "    second_best_prob = y_prob[idx, second_best_class]\n",
    "    \n",
    "    tp_data.append({\n",
    "        'index': idx,\n",
    "        'disturbed_prob': disturbed_prob,\n",
    "        'second_best_class': class_names[second_best_class],\n",
    "        'second_best_prob': second_best_prob,\n",
    "        'all_probs': y_prob[idx].copy()\n",
    "    })\n",
    "\n",
    "# ============= ANALYZE FALSE POSITIVES (INCORRECT DISTURBED PREDICTIONS) =============\n",
    "false_positive_mask = ~disturbed_true_mask & disturbed_pred_mask\n",
    "\n",
    "# Get indices of false positives\n",
    "fp_indices = np.where(false_positive_mask)[0]\n",
    "\n",
    "# Calculate statistics for false positives\n",
    "fp_disturbed_probs = y_prob[fp_indices, DISTURBED_ID]\n",
    "\n",
    "print(f\"\\n=== False Positive Analysis (Incorrect Disturbed Predictions) ===\")\n",
    "print(f\"Total false positives: {len(fp_indices)}\")\n",
    "print(f\"\\n=== Disturbed Probability Statistics for False Positives ===\")\n",
    "print(f\"Mean: {np.mean(fp_disturbed_probs):.4f}\")\n",
    "print(f\"Median: {np.median(fp_disturbed_probs):.4f}\")\n",
    "print(f\"Min: {np.min(fp_disturbed_probs):.4f}\")\n",
    "print(f\"Max: {np.max(fp_disturbed_probs):.4f}\")\n",
    "print(f\"Std: {np.std(fp_disturbed_probs):.4f}\")\n",
    "\n",
    "# Analyze which classes were incorrectly predicted as disturbed\n",
    "true_classes_fp = y_true[fp_indices]\n",
    "unique_fp, counts_fp = np.unique(true_classes_fp, return_counts=True)\n",
    "\n",
    "print(f\"\\n=== False Positives: True Classes ===\")\n",
    "for cls, count in zip(unique_fp, counts_fp):\n",
    "    print(f\"{class_names[cls]}: {count}\")\n",
    "\n",
    "# Store false positive data\n",
    "fp_data = []\n",
    "for idx in fp_indices:\n",
    "    true_class = class_names[y_true[idx]]\n",
    "    disturbed_prob = y_prob[idx, DISTURBED_ID]\n",
    "    true_class_prob = y_prob[idx, y_true[idx]]\n",
    "    \n",
    "    fp_data.append({\n",
    "        'index': idx,\n",
    "        'true_class': true_class,\n",
    "        'disturbed_prob': disturbed_prob,\n",
    "        'true_class_prob': true_class_prob,\n",
    "        'all_probs': y_prob[idx].copy()\n",
    "    })\n",
    "\n",
    "# Save analysis data\n",
    "os.makedirs(\"disturbed_analysis\", exist_ok=True)\n",
    "with open(\"disturbed_analysis/tp_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tp_data, f)\n",
    "with open(\"disturbed_analysis/fp_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(fp_data, f)\n",
    "print(f\"\\nSaved true positive data to disturbed_analysis/tp_data.pkl\")\n",
    "print(f\"Saved false positive data to disturbed_analysis/fp_data.pkl\")\n",
    "\n",
    "# ============= ANALYZE FALSE NEGATIVES (from earlier) =============\n",
    "false_negative_mask = disturbed_true_mask & ~disturbed_pred_mask\n",
    "fn_indices = np.where(false_negative_mask)[0]\n",
    "fn_disturbed_probs = y_prob[fn_indices, DISTURBED_ID]\n",
    "\n",
    "print(f\"\\n=== False Negative Analysis (Missed Disturbed Galaxies) ===\")\n",
    "print(f\"Total false negatives: {len(fn_indices)}\")\n",
    "print(f\"\\n=== Disturbed Probability Statistics for False Negatives ===\")\n",
    "print(f\"Mean: {np.mean(fn_disturbed_probs):.4f}\")\n",
    "print(f\"Median: {np.median(fn_disturbed_probs):.4f}\")\n",
    "print(f\"Min: {np.min(fn_disturbed_probs):.4f}\")\n",
    "print(f\"Max: {np.max(fn_disturbed_probs):.4f}\")\n",
    "print(f\"Std: {np.std(fn_disturbed_probs):.4f}\")\n",
    "\n",
    "# ============= COMPREHENSIVE COMPARISON =============\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(f\"=== COMPREHENSIVE COMPARISON: TP vs FP vs FN ===\")\n",
    "print(f\"{'='*90}\")\n",
    "print(f\"{'Metric':<35} {'True Positives':<18} {'False Positives':<18} {'False Negatives':<18}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "print(f\"{'Count':<35} {len(tp_disturbed_probs):<18} {len(fp_disturbed_probs):<18} {len(fn_disturbed_probs):<18}\")\n",
    "print(f\"{'Mean Disturbed Prob':<35} {np.mean(tp_disturbed_probs):<18.4f} {np.mean(fp_disturbed_probs):<18.4f} {np.mean(fn_disturbed_probs):<18.4f}\")\n",
    "print(f\"{'Median Disturbed Prob':<35} {np.median(tp_disturbed_probs):<18.4f} {np.median(fp_disturbed_probs):<18.4f} {np.median(fn_disturbed_probs):<18.4f}\")\n",
    "print(f\"{'Min Disturbed Prob':<35} {np.min(tp_disturbed_probs):<18.4f} {np.min(fp_disturbed_probs):<18.4f} {np.min(fn_disturbed_probs):<18.4f}\")\n",
    "print(f\"{'Max Disturbed Prob':<35} {np.max(tp_disturbed_probs):<18.4f} {np.max(fp_disturbed_probs):<18.4f} {np.max(fn_disturbed_probs):<18.4f}\")\n",
    "print(f\"{'Std Disturbed Prob':<35} {np.std(tp_disturbed_probs):<18.4f} {np.std(fp_disturbed_probs):<18.4f} {np.std(fn_disturbed_probs):<18.4f}\")\n",
    "\n",
    "print(f\"\\n{'Interpretation:':<35}\")\n",
    "print(f\"  TP avg > threshold: Model confident & correct\")\n",
    "print(f\"  FP avg near threshold: Model uncertain, made mistakes\")\n",
    "print(f\"  FN avg < threshold: Model uncertain, missed detections\")\n",
    "\n",
    "# Probability distribution comparison plot - 3 subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# TP distribution\n",
    "axes[0].hist(tp_disturbed_probs, bins=30, alpha=0.7, edgecolor='black', color='green')\n",
    "axes[0].axvline(x=DISTURBED_THRESHOLD, color='r', linestyle='--', linewidth=2, \n",
    "                label=f'Threshold = {DISTURBED_THRESHOLD}')\n",
    "axes[0].axvline(x=np.mean(tp_disturbed_probs), color='blue', linestyle='-', linewidth=2, \n",
    "                label=f'Mean = {np.mean(tp_disturbed_probs):.3f}')\n",
    "axes[0].set_xlabel('Disturbed Class Probability', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_title(f'True Positives (n={len(tp_disturbed_probs)})\\nCorrect Disturbed Predictions', fontsize=12, color='green')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# FP distribution\n",
    "axes[1].hist(fp_disturbed_probs, bins=20, alpha=0.7, edgecolor='black', color='orange')\n",
    "axes[1].axvline(x=DISTURBED_THRESHOLD, color='r', linestyle='--', linewidth=2, \n",
    "                label=f'Threshold = {DISTURBED_THRESHOLD}')\n",
    "axes[1].axvline(x=np.mean(fp_disturbed_probs), color='blue', linestyle='-', linewidth=2, \n",
    "                label=f'Mean = {np.mean(fp_disturbed_probs):.3f}')\n",
    "axes[1].set_xlabel('Disturbed Class Probability', fontsize=12)\n",
    "axes[1].set_ylabel('Count', fontsize=12)\n",
    "axes[1].set_title(f'False Positives (n={len(fp_disturbed_probs)})\\nIncorrect Disturbed Predictions', fontsize=12, color='orange')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# FN distribution\n",
    "axes[2].hist(fn_disturbed_probs, bins=20, alpha=0.7, edgecolor='black', color='red')\n",
    "axes[2].axvline(x=DISTURBED_THRESHOLD, color='r', linestyle='--', linewidth=2, \n",
    "                label=f'Threshold = {DISTURBED_THRESHOLD}')\n",
    "axes[2].axvline(x=np.mean(fn_disturbed_probs), color='blue', linestyle='-', linewidth=2, \n",
    "                label=f'Mean = {np.mean(fn_disturbed_probs):.3f}')\n",
    "axes[2].set_xlabel('Disturbed Class Probability', fontsize=12)\n",
    "axes[2].set_ylabel('Count', fontsize=12)\n",
    "axes[2].set_title(f'False Negatives (n={len(fn_disturbed_probs)})\\nMissed Disturbed Galaxies', fontsize=12, color='red')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Disturbed Class Probability Distribution: TP vs FP vs FN', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"disturbed_analysis/tp_fp_fn_probability_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Combined overlay plot for direct comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(tp_disturbed_probs, bins=30, alpha=0.5, edgecolor='black', color='green', label=f'TP (n={len(tp_disturbed_probs)}, μ={np.mean(tp_disturbed_probs):.3f})')\n",
    "plt.hist(fp_disturbed_probs, bins=20, alpha=0.5, edgecolor='black', color='orange', label=f'FP (n={len(fp_disturbed_probs)}, μ={np.mean(fp_disturbed_probs):.3f})')\n",
    "plt.hist(fn_disturbed_probs, bins=20, alpha=0.5, edgecolor='black', color='red', label=f'FN (n={len(fn_disturbed_probs)}, μ={np.mean(fn_disturbed_probs):.3f})')\n",
    "plt.axvline(x=DISTURBED_THRESHOLD, color='black', linestyle='--', linewidth=2, label=f'Threshold = {DISTURBED_THRESHOLD}')\n",
    "plt.xlabel('Disturbed Class Probability', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.title('Overlaid Probability Distributions: True Positives vs False Positives vs False Negatives', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"disturbed_analysis/overlay_tp_fp_fn_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 4) Build confusion matrix\n",
    "num_classes = len(class_names) if class_names is not None else int(np.max(y_true)) + 1\n",
    "cm = np.zeros((num_classes, num_classes), dtype=np.int64)\n",
    "for t, p in zip(y_true, y_pred):\n",
    "    cm[int(t), int(p)] += 1\n",
    "\n",
    "# 5) Plot confusion matrix\n",
    "plt.figure(figsize=(9, 8))\n",
    "plt.imshow(cm, interpolation=\"nearest\", cmap='viridis')\n",
    "plt.title(f\"Confusion Matrix (Disturbed Threshold={DISTURBED_THRESHOLD})\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(num_classes)\n",
    "\n",
    "labels = class_names if class_names is not None else [str(i) for i in range(num_classes)]\n",
    "plt.xticks(tick_marks, labels, rotation=45, ha=\"right\")\n",
    "plt.yticks(tick_marks, labels)\n",
    "\n",
    "thresh = cm.max() / 2.0\n",
    "for i in range(num_classes):\n",
    "    for j in range(num_classes):\n",
    "        plt.text(\n",
    "            j, i, str(cm[i, j]),\n",
    "            ha=\"center\", va=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\"\n",
    "        )\n",
    "\n",
    "plt.ylabel(\"True label\")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6) Overall metrics\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "f1_macro    = f1_score(y_true, y_pred, average=\"macro\")\n",
    "f1_weighted = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "f1_micro    = f1_score(y_true, y_pred, average=\"micro\")\n",
    "\n",
    "prec_macro  = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "rec_macro   = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "print(f\"\\nAccuracy:     {acc:.4f}\")\n",
    "print(f\"F1 macro:     {f1_macro:.4f}\")\n",
    "print(f\"F1 weighted:  {f1_weighted:.4f}\")\n",
    "print(f\"F1 micro:     {f1_micro:.4f}\")\n",
    "print(f\"Precision(m): {prec_macro:.4f}\")\n",
    "print(f\"Recall(m):    {rec_macro:.4f}\")\n",
    "\n",
    "# 7) Per-class report\n",
    "print(\"\\nClassification report:\\n\")\n",
    "print(classification_report(\n",
    "    y_true, y_pred,\n",
    "    target_names=class_names,\n",
    "    digits=4,\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "# 8) Show disturbed class metrics specifically\n",
    "disturbed_pred = (y_pred == DISTURBED_ID)\n",
    "\n",
    "tp = np.sum(disturbed_true_mask & disturbed_pred)\n",
    "fp = np.sum(~disturbed_true_mask & disturbed_pred)\n",
    "fn = np.sum(disturbed_true_mask & ~disturbed_pred)\n",
    "\n",
    "precision_disturbed = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall_disturbed = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1_disturbed = 2 * precision_disturbed * recall_disturbed / (precision_disturbed + recall_disturbed) if (precision_disturbed + recall_disturbed) > 0 else 0\n",
    "\n",
    "print(f\"\\n=== Disturbed Class Detailed Metrics ===\")\n",
    "print(f\"True Positives: {tp}\")\n",
    "print(f\"False Positives: {fp}\")\n",
    "print(f\"False Negatives: {fn}\")\n",
    "print(f\"Precision: {precision_disturbed:.4f}\")\n",
    "print(f\"Recall: {recall_disturbed:.4f}\")\n",
    "print(f\"F1-Score: {f1_disturbed:.4f}\")\n",
    "print(f\"\\nAverage Disturbed Probability:\")\n",
    "print(f\"  True Positives:  {np.mean(tp_disturbed_probs):.4f} (Confident & Correct)\")\n",
    "print(f\"  False Positives: {np.mean(fp_disturbed_probs):.4f} (Uncertain, Made Mistake)\")\n",
    "print(f\"  False Negatives: {np.mean(fn_disturbed_probs):.4f} (Uncertain, Missed Detection)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d867105d-bf6f-42bd-a090-e5df4ce3e4f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b294ca29-b0a9-4d4e-ba18-59f108757b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
