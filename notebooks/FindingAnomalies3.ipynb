{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db739b33-ee38-42d5-8a8e-319c4d1d5e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, shutil, uuid\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from glob import glob\n",
    "from tensorflow import keras as K\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aa9775-1839-4341-84c4-25bedcfe159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Builds nearest-neighbor index so it can be used later to, transform new embeddings\n",
    "#find neighbors for new inputs, and use weighted vote to assign a cluster.\n",
    "#Use this so we don't need to always recreate HDBscan.\n",
    "#Run once whenever HDBscan is recreated\n",
    "\n",
    "\n",
    "#Creates reference points\n",
    "os.makedirs(\"artifacts/embeddings\", exist_ok=True)\n",
    "os.makedirs(\"artifacts/models\", exist_ok=True)\n",
    "\n",
    "# Load\n",
    "emb = np.load(\"artifacts/embeddings/galaxy10_embeddings.npy\").astype(np.float32)\n",
    "df  = pd.read_csv(\"artifacts/results/galaxy10_clustered.csv\")\n",
    "\n",
    "scaler = joblib.load(\"artifacts/models/scaler.pkl\")\n",
    "pca    = joblib.load(\"artifacts/models/pca20.pkl\")  # adjust if yours is named differently\n",
    "\n",
    "# Transform to clustering space\n",
    "Z  = scaler.transform(emb)\n",
    "Zp = pca.transform(Z).astype(np.float32)\n",
    "\n",
    "# Save reference arrays\n",
    "np.save(\"artifacts/embeddings/Zp_ref.npy\", Zp)\n",
    "np.save(\"artifacts/embeddings/cluster_id_ref.npy\", df[\"cluster_id\"].values.astype(np.int32))\n",
    "\n",
    "# Fit kNN and save\n",
    "K = 15\n",
    "knn = NearestNeighbors(n_neighbors=K, metric=\"euclidean\")\n",
    "knn.fit(Zp)\n",
    "joblib.dump(knn, \"artifacts/models/knn.pkl\")\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" - artifacts/embeddings/Zp_ref.npy\", Zp.shape)\n",
    "print(\" - artifacts/embeddings/cluster_id_ref.npy\", df['cluster_id'].shape)\n",
    "print(\" - artifacts/models/knn.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "Zp = np.load(\"artifacts/embeddings/Zp_ref.npy\").astype(np.float32)\n",
    "cluster_ids = np.load(\"artifacts/embeddings/cluster_id_ref.npy\").astype(np.int32)\n",
    "knn = joblib.load(\"artifacts/models/knn.pkl\")\n",
    "\n",
    "dists, _ = knn.kneighbors(Zp)\n",
    "\n",
    "# mean distance to neighbors 2..K (skip 0th self-distance)\n",
    "mean_knn_dist = dists[:, 1:].mean(axis=1)\n",
    "\n",
    "# Only use non-noise points to set a threshold\n",
    "base = mean_knn_dist[cluster_ids != -1]\n",
    "\n",
    "thr = float(np.percentile(base, 99))  # try 97–99 for stricter/looser\n",
    "np.save(\"artifacts/models/knn_dist_threshold.npy\", np.array([thr], dtype=np.float32))\n",
    "\n",
    "print(\"Saved artifacts/models/knn_dist_threshold.npy =\", thr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87e92b2-fb8f-4357-9862-5e1bdfa0ff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing centroids, and distance distrubutions from reference set, which is used to score new points\n",
    "#Run once whenever HDBscan is recreated\n",
    "\n",
    "Zp_ref = np.load(\"artifacts/embeddings/Zp_ref.npy\").astype(np.float32)\n",
    "cluster_ref = np.load(\"artifacts/embeddings/cluster_id_ref.npy\").astype(np.int32)\n",
    "\n",
    "#centroids and per-cluster distance distributions\n",
    "centroids = {}\n",
    "dist_dists = {}\n",
    "\n",
    "for cid in sorted(set(cluster_ref)):\n",
    "    if cid == -1:\n",
    "        continue\n",
    "    pts = Zp_ref[cluster_ref == cid]\n",
    "    c = pts.mean(axis=0)\n",
    "    centroids[cid] = c\n",
    "\n",
    "    d = np.linalg.norm(pts - c, axis=1)\n",
    "    dist_dists[cid] = d\n",
    "\n",
    "#Save for later use\n",
    "np.save(\"artifacts/models/cluster_centroids.npy\", centroids, allow_pickle=True)\n",
    "np.save(\"artifacts/models/cluster_centroid_dists.npy\", dist_dists, allow_pickle=True)\n",
    "\n",
    "print(\"Saved centroids + distance distributions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02df8f6-e206-40b6-a54d-f9f1d8869473",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load necessary files and helper methods to add new images\n",
    "#Run when adding new images\n",
    "\n",
    "knn = joblib.load(\"artifacts/models/knn.pkl\")\n",
    "cluster_ref = np.load(\"artifacts/embeddings/cluster_id_ref.npy\").astype(np.int32)\n",
    "thr = float(np.load(\"artifacts/models/knn_dist_threshold.npy\")[0])\n",
    "centroids = np.load(\"artifacts/models/cluster_centroids.npy\", allow_pickle=True).item()\n",
    "dist_dists = np.load(\"artifacts/models/cluster_centroid_dists.npy\", allow_pickle=True).item()\n",
    "\n",
    "\n",
    "\n",
    "# Cluster meaning map (dominant Galaxy10 class per cluster)\n",
    "cluster_map = pd.read_csv(\"artifacts/results/cluster_map.csv\").set_index(\"cluster_id\")\n",
    "\n",
    "def assign_cluster_knn(zp_new, vote_min=0.6):\n",
    "    \"\"\"\n",
    "    zp_new: (d,) float32 point in PCA space\n",
    "    Returns a dict with cluster_id, name, confidence, distance, anomaly flag.\n",
    "    \"\"\"\n",
    "    zp_new = np.asarray(zp_new, dtype=np.float32).reshape(1, -1)\n",
    "    dists, idxs = knn.kneighbors(zp_new)\n",
    "\n",
    "    neigh = idxs[0]\n",
    "    neigh_clusters = cluster_ref[neigh]\n",
    "\n",
    "    mean_dist = float(dists[0].mean())\n",
    "\n",
    "    # vote ignoring noise neighbors\n",
    "    valid = neigh_clusters[neigh_clusters != -1]\n",
    "    if len(valid) == 0:\n",
    "        return {\n",
    "            \"assigned_cluster_id\": -1,\n",
    "            \"cluster_name\": \"Noise/Unknown\",\n",
    "            \"vote_conf\": 0.0,\n",
    "            \"mean_knn_dist\": mean_dist,\n",
    "            \"is_anomaly\": True,\n",
    "        }\n",
    "\n",
    "    vals, counts = np.unique(valid, return_counts=True)\n",
    "    best = int(vals[np.argmax(counts)])\n",
    "    vote_conf = float(np.max(counts) / len(valid))\n",
    "\n",
    "\n",
    "    # distance to centroid + percentile\n",
    "    if best in centroids:\n",
    "        c = centroids[best]\n",
    "        dist_to_center = float(np.linalg.norm(zp_new.reshape(-1) - c))\n",
    "    \n",
    "        ref_d = dist_dists[best]\n",
    "        # percentile: fraction of reference points farther than this (higher = more central)\n",
    "        centrality = float((ref_d > dist_to_center).mean())  # 0..1\n",
    "    else:\n",
    "        dist_to_center = float(\"nan\")\n",
    "        centrality = float(\"nan\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # anomaly decision: far OR weak vote\n",
    "    is_anom = (mean_dist > thr) or (vote_conf < vote_min)\n",
    "\n",
    "    # map to human meaning\n",
    "    if best in cluster_map.index:\n",
    "        name = str(cluster_map.loc[best, \"dominant_name\"])\n",
    "    else:\n",
    "        name = f\"Cluster {best}\"\n",
    "\n",
    "        \n",
    "\n",
    "    return {\n",
    "        \"assigned_cluster_id\": best,\n",
    "        \"cluster_name\": name,\n",
    "        \"vote_conf\": vote_conf,\n",
    "        \"mean_knn_dist\": mean_dist,\n",
    "        \"dist_to_center\": dist_to_center,\n",
    "        \"center_centrality\": centrality,  # e.g. 0.82 means \"more central than 82% of cluster\"\n",
    "        \"is_anomaly\": bool(is_anom),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003c4bbd-6df3-4789-b2e0-d0de2ae5ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run to find anomalies within galaxy10_clustered.csv and save png with id\n",
    "#Galaxy10_clustered.csv only contains galaxy10decals dataset\n",
    "#As model improves less anamolies show up\n",
    "\n",
    "H5_PATH  = \"images/Galaxy10_DECals.h5\"\n",
    "CSV_PATH = \"artifacts/results/galaxy10_clustered.csv\"\n",
    "\n",
    "OUT_DIR = \"reports/anomalies_pages/cluster_grids\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "COLS = 8\n",
    "PER_PAGE = 64\n",
    "\n",
    "# Define what counts as an \"HDBSCAN anomaly\"\n",
    "INCLUDE_NOISE = True                 #cluster_id == -1\n",
    "LOWP_PERCENTILE = 5                  #bottom 5% membership_prob (non-noise)\n",
    "\n",
    "LOWP_CUTOFF = None                   \n",
    "\n",
    "\n",
    "def load_images_by_indices(idxs):\n",
    "    \"\"\"h5py fancy indexing needs increasing order; return images in original order.\"\"\"\n",
    "    idxs = np.asarray(idxs, dtype=np.int64)\n",
    "    order = np.argsort(idxs)\n",
    "    idxs_sorted = idxs[order]\n",
    "    with h5py.File(H5_PATH, \"r\") as f:\n",
    "        X = f[\"images\"]\n",
    "        imgs_sorted = X[idxs_sorted]\n",
    "    inv = np.argsort(order)\n",
    "    return imgs_sorted[inv]\n",
    "\n",
    "def save_grid(imgs, rows_df, out_path, title):\n",
    "    n = len(imgs)\n",
    "    rows = math.ceil(n / COLS)\n",
    "    plt.figure(figsize=(COLS*2, rows*2))\n",
    "\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(rows, COLS, i + 1)\n",
    "        ax.imshow(imgs[i])\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        r = rows_df.iloc[i]\n",
    "        idx  = int(r[\"idx\"])\n",
    "        y    = int(r[\"true_label\"])\n",
    "        cid  = int(r[\"cluster_id\"])\n",
    "        prob = float(r[\"membership_prob\"])\n",
    "\n",
    "        ax.set_title(f\"id={idx}\\ny={y} c={cid}\\np={prob:.2f}\", fontsize=8)\n",
    "\n",
    "    plt.suptitle(title, y=1.01, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"Saved\", out_path)\n",
    "\n",
    "\n",
    "#Load clustering results\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "\n",
    "#Build all current anomalies in HDBSCAN\n",
    "#Noise points are always anomalies\n",
    "#Plus low-membership points among clustered samples\n",
    "anoms = []\n",
    "\n",
    "if INCLUDE_NOISE:\n",
    "    noise = df[df[\"cluster_id\"] == -1].copy()\n",
    "    noise[\"anom_reason\"] = \"noise\"\n",
    "    noise[\"anom_score\"] = 2.0 + (1.0 - noise[\"membership_prob\"])  # prioritize noise\n",
    "    anoms.append(noise)\n",
    "\n",
    "non_noise = df[df[\"cluster_id\"] != -1].copy()\n",
    "\n",
    "if LOWP_CUTOFF is not None:\n",
    "    lowp = non_noise[non_noise[\"membership_prob\"] < float(LOWP_CUTOFF)].copy()\n",
    "    lowp[\"anom_reason\"] = f\"low_prob<{LOWP_CUTOFF}\"\n",
    "else:\n",
    "    #percentile-based cutoff\n",
    "    cutoff = float(np.percentile(non_noise[\"membership_prob\"], LOWP_PERCENTILE))\n",
    "    lowp = non_noise[non_noise[\"membership_prob\"] <= cutoff].copy()\n",
    "    lowp[\"anom_reason\"] = f\"low_prob<=p{LOWP_PERCENTILE}({cutoff:.4f})\"\n",
    "\n",
    "lowp[\"anom_score\"] = (1.0 - lowp[\"membership_prob\"])\n",
    "anoms.append(lowp)\n",
    "\n",
    "anom_df = pd.concat(anoms, ignore_index=True)\n",
    "\n",
    "#Sort anomalies: strongest first\n",
    "anom_df = anom_df.sort_values([\"anom_score\", \"membership_prob\"], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "#Save anomaly list CSV\n",
    "anom_csv = \"artifacts/results/hdbscan_anomalies_all.csv\"\n",
    "os.makedirs(\"artifacts/results\", exist_ok=True)\n",
    "anom_df[[\"idx\",\"true_label\",\"cluster_id\",\"membership_prob\",\"anom_reason\",\"anom_score\"]].to_csv(anom_csv, index=False)\n",
    "\n",
    "print(\"\\nTotal anomalies saved:\", len(anom_df))\n",
    "print(\"Saved anomaly table:\", anom_csv)\n",
    "print(\"Output images folder:\", OUT_DIR)\n",
    "\n",
    "#Save anamolies images\n",
    "total = len(anom_df)\n",
    "if total == 0:\n",
    "    print(\"No anomalies found with current settings.\")\n",
    "else:\n",
    "    for start in range(0, total, PER_PAGE):\n",
    "        end = min(start + PER_PAGE, total)\n",
    "        page = anom_df.iloc[start:end].copy().reset_index(drop=True)\n",
    "\n",
    "        imgs = load_images_by_indices(page[\"idx\"].values)\n",
    "\n",
    "        page_num = start // PER_PAGE + 1\n",
    "        out_path = os.path.join(OUT_DIR, f\"anomalies_page_{page_num:02d}_{start}_{end-1}.png\")\n",
    "\n",
    "        title = f\"HDBSCAN anomalies (page {page_num}) — {start}..{end-1} of {total}\"\n",
    "        save_grid(imgs, page, out_path, title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9980b71e-b68a-4a28-bb89-000a11345eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper methods for kNN and loading transforms, encoder, cluster map\n",
    "\n",
    "\n",
    "@K.utils.register_keras_serializable()\n",
    "class CastToFloat16(K.layers.Layer):\n",
    "    def call(self, x):\n",
    "        x = tf.cast(x, tf.float16)\n",
    "        return preprocess_input(x)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "INCOMING_DIR = Path(\"data/incoming/images\")\n",
    "ANOM_DIR     = Path(\"data/anomalies/images\")\n",
    "INCOMING_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ANOM_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LOG_PATH = Path(\"artifacts/results/new_inference.csv\")\n",
    "LOG_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#Load transforms\n",
    "scaler = joblib.load(\"artifacts/models/scaler.pkl\")\n",
    "pca    = joblib.load(\"artifacts/models/pca20.pkl\")\n",
    "\n",
    "#Load classifier -> encoder\n",
    "clf = tf.keras.models.load_model(\"best_model/galaxy_b3_final_BEST_100TP.keras\", compile=False)\n",
    "encoder = tf.keras.Model(clf.input, clf.layers[-2].output)\n",
    "\n",
    "#Load cluster meaning map\n",
    "cluster_map = pd.read_csv(\"artifacts/results/cluster_map.csv\").set_index(\"cluster_id\")\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_image(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "    img = tf.image.resize(img, (256, 256), method=\"bilinear\")\n",
    "    img = tf.cast(img, tf.float32)  # 0..255\n",
    "    return tf.expand_dims(img, 0)\n",
    "\n",
    "def infer_and_route(image_path, vote_min=0.6):\n",
    "    \"\"\"\n",
    "    Runs inference, assigns cluster via kNN, routes image into incoming/anomalies dataset,\n",
    "    and appends a row to artifacts/results/new_inference.csv.\n",
    "    \"\"\"\n",
    "    image_path = Path(image_path)\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    uid = uuid.uuid4().hex[:10]\n",
    "    ext = image_path.suffix.lower() if image_path.suffix else \".png\"\n",
    "\n",
    "    #Embed\n",
    "    x = preprocess_image(str(image_path))\n",
    "    e = encoder(x, training=False).numpy().astype(np.float32)  # (1,48)\n",
    "\n",
    "    #Project to clustering space\n",
    "    zp = pca.transform(scaler.transform(e)).astype(np.float32).reshape(-1)\n",
    "\n",
    "    #Assign cluster + anomaly (uses assign_cluster_knn)\n",
    "    out = assign_cluster_knn(zp, vote_min=vote_min)  \n",
    "\n",
    "    cid = int(out[\"assigned_cluster_id\"])\n",
    "    is_anom = bool(out[\"is_anomaly\"])\n",
    "\n",
    "    #Map to labels\n",
    "    if cid in cluster_map.index:\n",
    "        cluster_name = str(cluster_map.loc[cid, \"dominant_name\"])\n",
    "        cluster_purity = float(cluster_map.loc[cid, \"purity\"])\n",
    "    else:\n",
    "        cluster_name = \"Noise/Unknown\" if cid == -1 else f\"Cluster {cid}\"\n",
    "        cluster_purity = np.nan\n",
    "\n",
    "\n",
    "    #Route file\n",
    "    dest_dir = ANOM_DIR if is_anom else INCOMING_DIR\n",
    "    \n",
    "    orig_name = image_path.name          \n",
    "    dest_path = dest_dir / orig_name\n",
    "    \n",
    "    #avoid overwriting if already routed\n",
    "    if not dest_path.exists():\n",
    "        shutil.copy2(image_path, dest_path)\n",
    "\n",
    "\n",
    "    row = {\n",
    "        \"timestamp\": ts,\n",
    "        \"source_path\": str(image_path),\n",
    "        \"stored_path\": str(dest_path),\n",
    "        \"assigned_cluster_id\": cid,\n",
    "        \"cluster_name\": cluster_name,\n",
    "        \"cluster_purity\": cluster_purity,\n",
    "        \"vote_conf\": float(out[\"vote_conf\"]),\n",
    "        \"mean_knn_dist\": float(out[\"mean_knn_dist\"]),\n",
    "        \"is_anomaly\": is_anom,\n",
    "    }\n",
    "\n",
    "    #Append to CSV\n",
    "    if LOG_PATH.exists():\n",
    "        pd.DataFrame([row]).to_csv(LOG_PATH, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        pd.DataFrame([row]).to_csv(LOG_PATH, index=False)\n",
    "\n",
    "    return row\n",
    "\n",
    "# result = infer_and_route(test_path)\n",
    "\n",
    "# print(\n",
    "#     f\"cluster_id={result['assigned_cluster_id']} | \"\n",
    "#     f\"name={result['cluster_name']} | \"\n",
    "#     f\"anom={result['is_anomaly']} | \"\n",
    "#     f\"conf={result['vote_conf']:.2f} | \"\n",
    "#     f\"dist={result['mean_knn_dist']:.4f}\"\n",
    "# )\n",
    "# print(\"Saved to:\", result[\"stored_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b18b57b-a1a1-4df7-a8ee-f06f90839a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(clf.layers[:5]):\n",
    "    print(i, layer.name, layer.__class__.__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23337bfe-e58d-4862-8456-24e1d8a7cb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use kNN to run new data instead of recreating HDBscan every time\n",
    "\n",
    "paths = sorted(glob(\"data/new_drop/*.jpg\")) + sorted(glob(\"data/new_drop/*.png\")) + sorted(glob(\"data/new_drop/*.jpeg\"))\n",
    "print(\"Found images:\", len(paths))\n",
    "\n",
    "results = []\n",
    "fails = []\n",
    "\n",
    "for p in paths:\n",
    "    try:\n",
    "        row = infer_and_route(p) \n",
    "        results.append(row)\n",
    "        print(f\"{p.split('/')[-1]} -> {row['cluster_name']} | anom={row['is_anomaly']} | conf={row['vote_conf']:.2f}\")\n",
    "    except Exception as e:\n",
    "        fails.append({\"path\": p, \"error\": str(e)})\n",
    "        print(\"FAIL\", p, e)\n",
    "\n",
    "#Save a snapshot of run\n",
    "pd.DataFrame(results).to_csv(\"artifacts/results/new_inference_last_run.csv\", index=False)\n",
    "pd.DataFrame(fails).to_csv(\"artifacts/results/new_inference_failures.csv\", index=False)\n",
    "\n",
    "print(\"\\nDone.\")\n",
    "print(\"Processed:\", len(results))\n",
    "print(\"Failed:\", len(fails))\n",
    "\n",
    "if results:\n",
    "    r = pd.DataFrame(results)\n",
    "    print(\"\\nCluster counts:\")\n",
    "    print(r[\"cluster_name\"].value_counts())\n",
    "    print(\"\\nAnomalies:\", int(r[\"is_anomaly\"].sum()), \"out of\", len(r))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d27c6e-13bf-4b10-85ff-feb25608e205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
